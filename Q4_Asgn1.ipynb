{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $Active=Confirmed-Deceased-Recovered$\n",
    "- [x] I have used Active cases = # of active cases on last day of week(overlapping)\n",
    "- [x] Here using overlapping weeks helps in smoothing data, since we have data points every 3 and 4 days, rather than every 7 days\n",
    "- [x] I have used Active cases = # of active cases on last day of month(this is 14th of next month): as months are from 15th of previous month to 14th of next month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note** run all cell in one go as I have used same variable names and they get updated as we go from district to state to overall, then if one runs previous code then comes error that this variable has different length etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import scipy.signal as signal ## new module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Q1 json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to open json file\n",
    "f=open('neighbor-districts-modified.json')\n",
    "# this function basically stores json type files into python dictionary\n",
    "dist_modified=json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get district names and ids form Q1 json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_list_from_json=[]\n",
    "for key in dist_modified:\n",
    "    district_list_from_json.append(key)\n",
    "district_list_from_json=np.array(district_list_from_json)\n",
    "district_list_from_json.sort()\n",
    "\n",
    "state_district_codes=[]\n",
    "for i in range(len(district_list_from_json)):\n",
    "    state_district_codes.append(district_list_from_json[i].split('/')[1])\n",
    "\n",
    "# district names - sample entry: churu\n",
    "district_names_from_json=[] \n",
    "\n",
    "# district ids - sample entry: Q1090006\n",
    "district_ids_from_json=[]\n",
    "\n",
    "#use split() function and specify the separator '/' . Remember default seperator is whitspace\n",
    "for i in range(len(district_list_from_json)):\n",
    "    district_names_from_json.append(district_list_from_json[i].split(\"/\")[0])\n",
    "    district_ids_from_json.append(district_list_from_json[i].split(\"/\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_ids_list={}\n",
    "for i in range(len(district_names_from_json)):\n",
    "    district_ids_list[district_ids_from_json[i]]=district_names_from_json[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating time ids\n",
    "- [x] Start date: 2020-03-15\n",
    "- [x] End date: 2021-08-14\n",
    "- [x] With overlapping of weeks we have total 148 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries for mapping dates to time ids\n",
    "# eg. for 2020-3-15, time_id_week is 1, time_id_month is 1, time_id_ is 1\n",
    "\n",
    "time_id_week = {}\n",
    "time_id_month = {}\n",
    "time_id_overall = {}\n",
    "\n",
    "date=datetime.date(2020,3,15)\n",
    "day=1\n",
    "even_list=[]\n",
    "odd_list=[]\n",
    "for i in range(200):\n",
    "    if (i+1)%2==0:\n",
    "        even_list.append(i+1)\n",
    "    else:\n",
    "        odd_list.append(i+1)\n",
    "while True:\n",
    "    # basically to cover overlapping weeks this part needs to be changed.\n",
    "    # for now we are proceeding. but change week ids according above definition of 7-DMA\n",
    "    list_week=[0,1,2,6]\n",
    "    if date.weekday() in list_week:\n",
    "        time_id_week[str(date)]=odd_list[int(np.ceil(day/7))-1]\n",
    "    else:\n",
    "        time_id_week[str(date)]=even_list[int(np.ceil(day/7))-1]\n",
    "        \n",
    "    if str(date)[0:4]=='2020':\n",
    "        if int(str(date)[8:10]) <15:\n",
    "            time_id_month[str(date)]=int(str(date)[5:7])-3\n",
    "        else:\n",
    "            time_id_month[str(date)]=int(str(date)[5:7])-2\n",
    "    else:\n",
    "        if int(str(date)[8:10]) <15:\n",
    "            time_id_month[str(date)]=int(str(date)[5:7])+9\n",
    "        else:\n",
    "            time_id_month[str(date)]=int(str(date)[5:7])+10\n",
    "\n",
    "    time_id_overall[str(date)]=1\n",
    "    \n",
    "    if date==datetime.date(2021,8,14):\n",
    "        break\n",
    "    day=day+1\n",
    "    date=date+datetime.timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read districts csv for cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv=pd.read_csv('districts.csv')\n",
    "data_csv=data_csv.drop('Tested',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv=data_csv.sort_values(['District','Date'])\n",
    "data_csv.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting intersection district names from districts csv and modified json(same as Q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_names_from_cases=[]\n",
    "district_ids_from_cases=[]\n",
    "district_uniques=np.array(np.unique(data_csv['District']))\n",
    "for i in range(len(district_ids_from_json)):\n",
    "    a=district_ids_from_json[i].split('_')\n",
    "    for j in range(len(district_uniques)):\n",
    "        if district_uniques[j]==a[1]:\n",
    "            district_ids_from_cases.append(district_ids_from_json[i])\n",
    "            district_names_from_cases.append(a[1])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "these many districts are there in districts.csv = 648\n",
      "these many districts are in both files intersection = 622\n"
     ]
    }
   ],
   "source": [
    "print(\"these many districts are there in districts.csv =\",len(district_uniques)+5) # for those common five names\n",
    "print(\"these many districts are in both files intersection =\" ,len(district_ids_from_cases)) # though they are included in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------To check whether things are consistent------\n",
      "MH_Beed\n",
      "Beed\n"
     ]
    }
   ],
   "source": [
    "print('-------To check whether things are consistent------')\n",
    "print(district_ids_from_cases[67])\n",
    "print(district_names_from_cases[67])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To get daily cases from cummulative confirmed cases\n",
    "- [x] Formula use: $Active=Confirmed-Deceased-Recovered$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv['Active']=data_csv['Confirmed']-(data_csv['Deceased']+data_csv['Recovered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Giving week and month ID to each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_in_raw=np.unique(data_csv['Date']).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.8 s, sys: 19.3 ms, total: 16.9 s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_csv['Week ID']=np.nan\n",
    "data_csv['Month ID']=np.nan\n",
    "for date in time_id_week:\n",
    "    if dates_in_raw.count(date)>0:\n",
    "        data_csv.loc[data_csv['Date']==date,'Week ID']=time_id_week[date]\n",
    "        data_csv.loc[data_csv['Date']==date,'Month ID']=time_id_month[date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping rows with out of range dates\n",
    "- [x] These many dates are after end date: 2021-08-14 (for all districts in intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             0\n",
       "State            0\n",
       "District         0\n",
       "Confirmed        0\n",
       "Recovered        0\n",
       "Deceased         0\n",
       "Other            0\n",
       "Active           0\n",
       "Week ID      12502\n",
       "Month ID     12502\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for Districts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Delhi also can be included but I dont't think I should write special code just to include one District."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Week timeline\n",
    "- [x] I have used Active cases = # of active cases on last day of week(overlapping)\n",
    "- [x] Here using overlapping weeks helps in smoothing data, since we have data points every 3 and 4 days, rather than every 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.8 s, sys: 24 ms, total: 43.8 s\n",
      "Wall time: 43.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "no_of_weeks=list(time_id_week.values())[-1]\n",
    "districtid=[]\n",
    "weekid=[]\n",
    "cases=[] #active cases\n",
    "for i in range(len(district_names_from_cases)):\n",
    "    data_foo=data_csv[data_csv['District']==district_names_from_cases[i]]\n",
    "    for j in range(no_of_weeks):\n",
    "        districtid.append(district_ids_from_cases[i])\n",
    "        weekid.append(j+1)\n",
    "        foo_df=data_foo[data_foo['Week ID']==j+2]\n",
    "        if foo_df.shape[0]==0:\n",
    "            cases.append(0)\n",
    "        else:\n",
    "            cases.append(foo_df.Active.tolist()[-1])\n",
    "        #active.append(foo_df['Active'].sum())\n",
    "        \n",
    "week_df=pd.DataFrame({'districtid':districtid,'weekid':weekid,'cases':cases})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that chooses between more than two peaks finded by $argrelextrema$\n",
    "- [x] discription is given in python comments\n",
    "- [x] there is one special case of MN_Churachandpur, where everything is zero, I have given it avg peak1 and peak2 values, manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_function_week(time_series,indices):\n",
    "    \n",
    "    indices=indices.tolist()\n",
    "    \n",
    "    foo_list=time_series[indices]\n",
    "    max1=max(foo_list.tolist()) # find first maxima\n",
    "    ind1=indices[foo_list.argmax()] # find fist maxima's index\n",
    "    indices.remove(ind1)\n",
    "    \n",
    "    time_series[ind1]=-99999\n",
    "    foo_list=time_series[indices]\n",
    "    max2=max(foo_list.tolist()) # find second maxima\n",
    "    if max1==max2: # to cover the cases where value of  peaks is equal and that gives nearby peak1 and peak2 values\n",
    "      ind_foo=indices[foo_list.argmax()]\n",
    "      indices.remove(ind_foo)\n",
    "      time_series[ind_foo]=-99999\n",
    "      foo_list=time_series[indices]\n",
    "      ind2=indices[foo_list.argmax()]\n",
    "    else:\n",
    "      ind2=indices[foo_list.argmax()] # same thing can be done for more than two max (but those cases are like 3-4), and rather complicates\n",
    "    \n",
    "    ### for cases like MN_Churachandpur\n",
    "    ### where there is no record of cases\n",
    "    if max1==0:# there is only one district like this MN_Churachandpur \n",
    "      ind1=50  # if remove this then this gives peaks some cakcy value\n",
    "      ind2=120 # for that giving a nice value around avg of other district peaks\n",
    "    \n",
    "    # return index\n",
    "    if ind1<ind2: # swap index when peak1_week > peak2_week\n",
    "        return [ind1,ind2]\n",
    "    else:\n",
    "        return [ind2,ind1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find peaks with $argrelextrema$\n",
    "- [x] I have used(trial and error) order $40$ and $20$ for $argrelextrema$ and it gives quite reasonable and nice results\n",
    "- [x] To give lower orders used if-else conditions on length of indices list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1_week=[]\n",
    "peak2_week=[]\n",
    "# week ID automatically becomes ''indices+1'' by adding one in indices\n",
    "\n",
    "for i in range(len(district_ids_from_cases)):\n",
    "    time_series=np.array(week_df[week_df['districtid']==district_ids_from_cases[i]].cases)\n",
    "    indices=signal.argrelextrema(time_series,np.greater_equal, order=40)[0]\n",
    "    if len(indices)==2:\n",
    "        peak1_week.append(indices[0]+1)\n",
    "        peak2_week.append(indices[1]+1)\n",
    "    else:\n",
    "        if len(indices)<2:\n",
    "            indices=signal.argrelextrema(time_series,np.greater_equal, order=20)[0]\n",
    "            if len(indices)==2:\n",
    "                peak1_week.append(indices[0]+1)\n",
    "                peak2_week.append(indices[1]+1)\n",
    "            else:\n",
    "                indices=choose_function_week(time_series,indices)\n",
    "                peak1_week.append(indices[0]+1)\n",
    "                peak2_week.append(indices[1]+1)\n",
    "        else:\n",
    "            indices=choose_function_week(time_series,indices)\n",
    "            peak1_week.append(indices[0]+1)\n",
    "            peak2_week.append(indices[1]+1)\n",
    "\n",
    "# 585 cases pass len==2 condition at fiirst go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique peak values analysis(weeks)\n",
    "- [x] there only seems 4-5 outliers in peak1 and peak2. Gives that $argrelextrema$ works just fine with appropriate order given to it.\n",
    "- [x] It is quite possible that these outlier peaks are due to discrepencies/errors in recording data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  1,  18,  20,  27,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
      "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
      "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
      "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
      "        79,  81,  87,  88, 121, 122]), array([ 1,  2,  1,  3,  1,  2,  3,  2,  3,  1,  3,  6, 10, 14, 14, 14, 29,\n",
      "       10, 16,  8,  8, 20, 20, 21, 29, 43, 50, 47, 36, 20, 27, 17, 17,  9,\n",
      "        7,  3, 11,  8,  9,  3,  6,  6,  5,  1,  4, 10, 16,  3,  1,  4,  9,\n",
      "        1,  1,  1,  1,  1,  2,  2]))\n",
      "(array([103, 107, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n",
      "       122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 135,\n",
      "       136, 137, 138, 139, 140, 141, 142, 144, 145, 146, 147]), array([ 1,  2,  2,  5,  6, 10, 24, 42, 51, 44, 76, 84, 73, 39, 35, 35, 27,\n",
      "       13,  5,  4,  4,  2,  2,  1,  1,  2,  3,  1,  3,  4,  3,  5,  1,  2,\n",
      "        5,  1,  4]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(peak1_week,return_counts=True))\n",
    "print(np.unique(peak2_week,return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month timeline\n",
    "- [x] I have used Active cases = # of active cases on last day of month(this is 14th of next month): as months are from 15th of previous month to 14th of next month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.8 s, sys: 0 ns, total: 13.8 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "no_of_months=list(time_id_month.values())[-1]\n",
    "districtid=[]\n",
    "monthid=[]\n",
    "cases=[]\n",
    "for i in range(len(district_names_from_cases)): # if delhi etc are not there just \n",
    "    data_foo=data_csv[data_csv['District']==district_names_from_cases[i]]\n",
    "    for j in range(no_of_months):\n",
    "        districtid.append(district_ids_from_cases[i])\n",
    "        monthid.append(j+1)\n",
    "        foo_df=data_foo[data_foo['Month ID']==j+1]\n",
    "        \n",
    "        if foo_df.shape[0]==0:\n",
    "            cases.append(0)\n",
    "        else:\n",
    "            cases.append(foo_df.Active.tolist()[-1])\n",
    "month_df=pd.DataFrame({'districtid':districtid,'monthid':monthid,'cases':cases})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function that chooses between more than two peaks finded by $argrelextrema$\n",
    "- [x] discription is given in python comments\n",
    "- [x] there is one special case of MN_Churachandpur, where everything is zero, I have given it avg peak1 and peak2 values, manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_function_month(time_series,indices):\n",
    "    indices=indices.tolist()\n",
    "     \n",
    "    # find index of first max\n",
    "    foo_list=time_series[indices]\n",
    "    max1=max(foo_list.tolist())\n",
    "    ind1=indices[foo_list.argmax()]\n",
    "    indices.remove(ind1)\n",
    "    time_series[ind1]=0\n",
    "    \n",
    "    #find index of second peak\n",
    "    foo_list=time_series[indices]\n",
    "    ind2=indices[foo_list.argmax()]\n",
    "    \n",
    "    ### for cases like MN_Churachandpur\n",
    "    ### where there is no record of cases\n",
    "    if max1==0:# there is only one district like this MN_Churachandpur \n",
    "      ind1=6  # if remove this then this gives peaks some cakcy value\n",
    "      ind2=13 # for that giving a nice value around avg of other district peak\n",
    "        \n",
    "    # return index\n",
    "    if ind1<ind2:\n",
    "        return [ind1,ind2]\n",
    "    else:\n",
    "        return [ind2,ind1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find peaks with $argrelextrema$\n",
    "- [x] I have used(trial and error) order $4$, $3$, and $2$ for $argrelextrema$ and it gives quite reasonable and nice results\n",
    "- [x] To give lower orders used if-else conditions on length of indices list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1_month=[]\n",
    "peak2_month=[]\n",
    "\n",
    "# month ID automatically becomes ''indices+1'' by adding 1 in indices\n",
    "for i in range(len(district_ids_from_cases)):\n",
    "    time_series=np.array(month_df[month_df['districtid']==district_ids_from_cases[i]].cases)\n",
    "    indices=signal.argrelextrema(time_series,np.greater_equal, order=4)[0]\n",
    "    if len(indices)==2:\n",
    "        peak1_month.append(indices[0]+1)\n",
    "        peak2_month.append(indices[1]+1)\n",
    "    else:\n",
    "        if len(indices)<2:\n",
    "            indices=signal.argrelextrema(time_series,np.greater_equal, order=3)[0]\n",
    "            if len(indices)==2:\n",
    "                peak1_month.append(indices[0]+1)\n",
    "                peak2_month.append(indices[1]+1)\n",
    "            else:\n",
    "                if len(indices)<2:\n",
    "                    indices=signal.argrelextrema(time_series,np.greater_equal, order=2)[0]\n",
    "                    if len(indices)==2:\n",
    "                        peak1_month.append(indices[0]+1)\n",
    "                        peak2_month.append(indices[1]+1)\n",
    "                    else:\n",
    "                        indices=choose_function_month(time_series,indices)\n",
    "                        peak1_month.append(indices[0]+1)\n",
    "                        peak2_month.append(indices[1]+1)\n",
    "                else:\n",
    "                    indices=choose_function_month(time_series,indices)\n",
    "                    peak1_month.append(indices[0]+1)\n",
    "                    peak2_month.append(indices[1]+1)\n",
    "        else:\n",
    "            indices=choose_function_month(time_series,indices)\n",
    "            peak1_month.append(indices[0]+1)\n",
    "            peak2_month.append(indices[1]+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique peak values analysis(months)\n",
    "- [x] there only seems 4-5 outliers in peak1 and peak2. Gives that $argrelextrema$ works just fine with appropriate order given to it.\n",
    "- [x] It is quite possible that these outlier peaks are due to discrepencies/errors in recording data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]), array([  1,   1,   3,  20, 124, 295, 102,  42,  30,   4]))\n",
      "(array([12, 13, 14, 15, 16, 17]), array([  1,  38, 506,  46,  21,  10]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(peak1_month,return_counts=True))\n",
    "print(np.unique(peak2_month,return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gives districtid list for data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_district_ids_list=[]\n",
    "for x in districtid:\n",
    "    # check if exists in unique_list or not\n",
    "    if x not in unique_district_ids_list:\n",
    "        unique_district_ids_list.append(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### converting overlapping weekid back to non-overlapping weekids\n",
    "- [x] overlapping($1-148$) to non-overlapping($1-74$)\n",
    "- [x] for odd overlapping week id add 1\n",
    "- [x] now devide by 2 to get non-overlapping weekis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1=[]\n",
    "peak2=[]\n",
    "for i in range(len(peak1_week)):\n",
    "    a=peak1_week[i]\n",
    "    b=peak2_week[i]\n",
    "    if a%2==0:\n",
    "        peak1.append(int(a/2))\n",
    "    else:\n",
    "        peak1.append(int((a+1)/2))\n",
    "    if b%2==0:\n",
    "        peak2.append(int(b/2))\n",
    "    else:\n",
    "        peak2.append(int((b+1)/2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write peaks(weeks and months) for districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_peaks_df=pd.DataFrame({'districtid':unique_district_ids_list,'wave1 − weekid':peak1, 'wave2 − weekid':peak2, 'wave1 − monthid':peak1_month, 'wave2 − monthid':peak2_month\n",
    "})\n",
    "district_peaks_df.sort_values(\"districtid\",axis=0,ascending=True, inplace=True, kind='mergesort')\n",
    "district_peaks_df.reset_index(inplace=True,drop=True)\n",
    "district_peaks_df.to_csv('district-peaks.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] I have used district only from week_df/month_df to avoid extra counting of districts that are not there in the intersection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add stateids to the week-df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list=[]\n",
    "for i in range(week_df.shape[0]):\n",
    "    state_list.append(week_df['districtid'][i].split('_')[0])\n",
    "    \n",
    "week_df['stateid']=state_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add stateids to the month-df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list=[]\n",
    "for i in range(month_df.shape[0]):\n",
    "    state_list.append(month_df['districtid'][i].split('_')[0])\n",
    "    \n",
    "month_df['stateid']=state_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get unique state IDs from week_df/month_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateid_unique=np.unique(np.array(week_df.stateid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cases counting for states from week_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for weeks\n",
    "stateid=[]\n",
    "weeks=[]\n",
    "case_count=[]\n",
    "for i in range(len(stateid_unique)):\n",
    "    data_foo=week_df[week_df['stateid']==stateid_unique[i]]\n",
    "    for j in range(no_of_weeks):\n",
    "        stateid.append(stateid_unique[i])\n",
    "        weeks.append(j+1)\n",
    "        foo_df=data_foo[data_foo['weekid']==j+1]\n",
    "        case_count.append(sum(foo_df.cases.tolist()))\n",
    "week_df=pd.DataFrame({'districtid':stateid,'weekid':weeks,'cases':case_count})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find peaks with $argrelextrema$\n",
    "- [x] I have used(trial and error) order $40$ and $20$ for $argrelextrema$ and it gives quite reasonable and nice results\n",
    "- [x] To give lower orders used if-else conditions on length of indices list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1_week=[]\n",
    "peak2_week=[]\n",
    "# week ID automatically becomes ''indices+1'' by adding one in indices\n",
    "\n",
    "for i in range(len(stateid_unique)):\n",
    "    time_series=np.array(week_df[week_df['districtid']==stateid_unique[i]].cases)\n",
    "    indices=signal.argrelextrema(time_series,np.greater_equal, order=30)[0]\n",
    "    if len(indices)==2:\n",
    "        peak1_week.append(indices[0]+1)\n",
    "        peak2_week.append(indices[1]+1)\n",
    "    else:\n",
    "        if len(indices)<2:\n",
    "            indices=signal.argrelextrema(time_series,np.greater_equal, order=25)[0]\n",
    "            if len(indices)==2:\n",
    "                peak1_week.append(indices[0]+1)\n",
    "                peak2_week.append(indices[1]+1)\n",
    "            else:\n",
    "                indices=choose_function_week(time_series,indices)\n",
    "                peak1_week.append(indices[0]+1)\n",
    "                peak2_week.append(indices[1]+1)\n",
    "        else:\n",
    "            indices=choose_function_week(time_series,indices)\n",
    "            peak1_week.append(indices[0]+1)\n",
    "            peak2_week.append(indices[1]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique peak values analysis(weeks)\n",
    "- [x] there only seems 4-5 outliers in peak1 and peak2. Gives that $argrelextrema$ works just fine with appropriate order given to it.\n",
    "- [x] It is quite possible that these outlier peaks are due to discrepencies/errors in recording data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1, 38, 43, 48, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 63, 64, 73]), array([1, 1, 3, 1, 1, 3, 2, 6, 2, 2, 1, 1, 2, 1, 1, 1, 2]))\n",
      "(array([114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 141,\n",
      "       144]), array([1, 1, 1, 1, 1, 4, 5, 7, 1, 1, 4, 2, 1, 1]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(peak1_week,return_counts=True))\n",
    "print(np.unique(peak2_week,return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cases counting for states from month_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for months\n",
    "stateid=[]\n",
    "weeks=[]\n",
    "case_count=[]\n",
    "for i in range(len(stateid_unique)):\n",
    "    data_foo=month_df[month_df['stateid']==stateid_unique[i]]\n",
    "    for j in range(no_of_months):\n",
    "        stateid.append(stateid_unique[i])\n",
    "        weeks.append(j+1)\n",
    "        foo_df=data_foo[data_foo['monthid']==j+1]\n",
    "        case_count.append(sum(foo_df.cases.tolist()))\n",
    "month_df=pd.DataFrame({'districtid':stateid,'monthid':weeks,'cases':case_count})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find peaks with $argrelextrema$\n",
    "- [x] I have used(trial and error) order $4$, $3$, and $2$ for $argrelextrema$ and it gives quite reasonable and nice results\n",
    "- [x] To give lower orders used if-else conditions on length of indices list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1_month=[]\n",
    "peak2_month=[]\n",
    "\n",
    "# month ID automatically becomes ''indices+1'' by adding 1 in indices\n",
    "for i in range(len(stateid_unique)):\n",
    "    time_series=np.array(month_df[month_df['districtid']==stateid_unique[i]].cases)\n",
    "    indices=signal.argrelextrema(time_series,np.greater_equal, order=4)[0]\n",
    "    if len(indices)==2:\n",
    "        peak1_month.append(indices[0]+1)\n",
    "        peak2_month.append(indices[1]+1)\n",
    "    else:\n",
    "        if len(indices)<2:\n",
    "            indices=signal.argrelextrema(time_series,np.greater_equal, order=3)[0]\n",
    "            if len(indices)==2:\n",
    "                peak1_month.append(indices[0]+1)\n",
    "                peak2_month.append(indices[1]+1)\n",
    "            else:\n",
    "                if len(indices)<2:\n",
    "                    indices=signal.argrelextrema(time_series,np.greater_equal, order=2)[0]\n",
    "                    if len(indices)==2:\n",
    "                        peak1_month.append(indices[0]+1)\n",
    "                        peak2_month.append(indices[1]+1)\n",
    "                    else:\n",
    "                        indices=choose_function_month(time_series,indices)\n",
    "                        peak1_month.append(indices[0]+1)\n",
    "                        peak2_month.append(indices[1]+1)\n",
    "                else:\n",
    "                    indices=choose_function_month(time_series,indices)\n",
    "                    peak1_month.append(indices[0]+1)\n",
    "                    peak2_month.append(indices[1]+1)\n",
    "        else:\n",
    "            indices=choose_function_month(time_series,indices)\n",
    "            peak1_month.append(indices[0]+1)\n",
    "            peak2_month.append(indices[1]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unique peak values analysis(months)\n",
    "- [x] there only seems 4-5 outliers in peak1 and peak2. Gives that $argrelextrema$ works just fine with appropriate order given to it.\n",
    "- [x] It is quite possible that these outlier peaks are due to discrepencies/errors in recording data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 5, 6, 7, 8, 9]), array([ 1,  4, 15,  9,  1,  1]))\n",
      "(array([13, 14, 15, 16, 17]), array([ 2, 25,  2,  1,  1]))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(peak1_month,return_counts=True))\n",
    "print(np.unique(peak2_month,return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### converting overlapping weekid back to non-overlapping weekids\n",
    "- [x] overlapping($1-148$) to non-overlapping($1-74$)\n",
    "- [x] for odd overlapping week id add 1\n",
    "- [x] now devide by 2 to get non-overlapping weekis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1=[]\n",
    "peak2=[]\n",
    "for i in range(len(peak1_week)):\n",
    "    a=peak1_week[i]\n",
    "    b=peak2_week[i]\n",
    "    if a%2==0:\n",
    "        peak1.append(int(a/2))\n",
    "    else:\n",
    "        peak1.append(int((a+1)/2))\n",
    "    if b%2==0:\n",
    "        peak2.append(int(b/2))\n",
    "    else:\n",
    "        peak2.append(int((b+1)/2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_peaks_df=pd.DataFrame({'stateid':stateid_unique,'wave1 − weekid':peak1, 'wave2 − weekid':peak2, 'wave1 − monthid':peak1_month, 'wave2 − monthid':peak2_month\n",
    "})\n",
    "state_peaks_df.sort_values(\"stateid\",axis=0,ascending=True, inplace=True, kind='mergesort')\n",
    "state_peaks_df.reset_index(inplace=True,drop=True)\n",
    "state_peaks_df.to_csv('state-peaks.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for Overall(India)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get cases count for overall(weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for weeks\n",
    "overallid='IN'\n",
    "weeks=[]\n",
    "case_count=[]\n",
    "for j in range(no_of_weeks):\n",
    "    stateid.append(overallid)\n",
    "    weeks.append(j+1)\n",
    "    case_count.append(sum(week_df[week_df.weekid==j+1].cases.tolist()))\n",
    "week_df=pd.DataFrame({'districtid':overallid,'weekid':weeks,'cases':case_count})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find peaks with $argrelextrema$\n",
    "- [x] I have used(trial and error) order $40$ for $argrelextrema$ and it gives quite reasonable and nice results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1_week=[]\n",
    "peak2_week=[]\n",
    "\n",
    "time_series=np.array(week_df[week_df['districtid']==overallid].cases)\n",
    "indices=signal.argrelextrema(time_series,np.greater_equal, order=40)[0]\n",
    "if len(indices)==2:\n",
    "    peak1_week.append(indices[0]+1)\n",
    "    peak2_week.append(indices[1]+1)\n",
    "else:\n",
    "    indices=choose_function_week(time_series,indices)\n",
    "    peak1_week.append(indices[0]+1)\n",
    "    peak2_week.append(indices[1]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### converting overlapping weekid back to non-overlapping weekids\n",
    "- [x] overlapping($1-148$) to non-overlapping($1-74$)\n",
    "- [x] for odd overlapping week id add 1\n",
    "- [x] now devide by 2 to get non-overlapping weekis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak1=[]\n",
    "peak2=[]\n",
    "for i in range(len(peak1_week)):\n",
    "    a=peak1_week[i]\n",
    "    b=peak2_week[i]\n",
    "    if a%2==0:\n",
    "        peak1.append(int(a/2))\n",
    "    else:\n",
    "        peak1.append(int((a+1)/2))\n",
    "    if b%2==0:\n",
    "        peak2.append(int(b/2))\n",
    "    else:\n",
    "        peak2.append(int((b+1)/2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get cases count for overall(months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for months\n",
    "overallid='IN'\n",
    "weeks=[]\n",
    "case_count=[]\n",
    "for j in range(no_of_months):\n",
    "    stateid.append(overallid)\n",
    "    weeks.append(j+1)\n",
    "    case_count.append(sum(month_df[month_df.monthid==j+1].cases.tolist()))\n",
    "month_df=pd.DataFrame({'districtid':overallid,'monthid':weeks,'cases':case_count})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find peaks with $argrelextrema$\n",
    "- [x] I have used(trial and error) order $4$ for $argrelextrema$ and it gives quite reasonable and nice results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "peak1_month=[]\n",
    "peak2_month=[]\n",
    "\n",
    "time_series=np.array(month_df[month_df['districtid']==overallid].cases)\n",
    "indices=signal.argrelextrema(time_series,np.greater_equal, order=4)[0]\n",
    "if len(indices)==2:\n",
    "    peak1_month.append(indices[0]+1)\n",
    "    peak2_month.append(indices[1]+1)\n",
    "else:\n",
    "    indices=choose_function_month(time_series,indices)\n",
    "    peak1_month.append(indices[0]+1)\n",
    "    peak2_month.append(indices[1]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write in overall csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_peaks_df=pd.DataFrame({'overallid':overallid,'wave1 − weekid':peak1, 'wave2 − weekid':peak2, 'wave1 − monthid':peak1_month, 'wave2 − monthid':peak2_month\n",
    "})\n",
    "overall_peaks_df.to_csv('overall-peaks.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
